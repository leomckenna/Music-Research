{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# Define local path for saving files\n",
    "LOCAL_PATH = \"/Users/leomckenna/Desktop/Music Research/\"\n",
    "\n",
    "# Setup logging\n",
    "LOG_FILE = os.path.join(LOCAL_PATH, \"youtube_scraper.log\")\n",
    "logging.basicConfig(filename=LOG_FILE, level=logging.DEBUG, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# YouTube API setup with multiple API keys for rotation\n",
    "API_KEYS = [\n",
    "    \"AIzaSyD1UHGH2KyewEINzDdJCGoveW8nhLFIiKc\",\n",
    "    \"AIzaSyCkzJl0Rxm2eEOj_urGDyp3DEPDDtDeCIQ\",\n",
    "    \"AIzaSyB19jMObUcK5UpM9ZDt9HO7TOV8DX-KlBE\",\n",
    "    \"AIzaSyBFa02uluT2ZPTD8By8KxJi3QCkq-5jdCI\",\n",
    "    \"AIzaSyAFTpKwgJOWVUJweW_ajbvMI1d8YjsRVHU\",\n",
    "    \"AIzaSyBzMuNSl2JpCo4mf_3QfArCMMKlPBksrTs\",\n",
    "    \"AIzaSyB0jTpl2tvaiA_hVL6vyE98YSG_IKaJhfA\",\n",
    "    \"AIzaSyAr-INrfcxR9_6ahsB2Fb4vZSknNoHsJAY\",\n",
    "    \"AIzaSyBD7N7HPKBI8cKLvlNkNHvJn3t4WV2WW5I\",\n",
    "    \"AIzaSyC1uNVX2Pf3vRekAFujmTVTa82kqJEBvO0\",\n",
    "    \"AIzaSyDTuyaUwYsWtsoPTEv2wgMAJHgzF1IBy4g\",\n",
    "    \"AIzaSyBMUpB9iTKixV94z9-ZUmcyDfT6tJ5QFVY\",\n",
    "    \"AIzaSyCPxHCGty4zadYFBt_9f031sIbL1Dd2zVA\",\n",
    "    \"AIzaSyC77YNfgnaZBqBwzGZ_ZI7z4XP6Ah8rCmc\",\n",
    "    \"AIzaSyBgNgfZfffPeuD1pcN92y-U-I7LJT3-Q-I\",\n",
    "    \"AIzaSyDYMROM-dn2Tg5Zh0NfiUq9VwmOVEPUSBI\",\n",
    "    \"AIzaSyDs4FuliVn3TE8rg0TPQE_zbYYQNDMYMHk\",\n",
    "    \"AIzaSyC7nbL3zSzowGaH0PrzqIPvTi-SYDvFioE\",\n",
    "    \"AIzaSyA9suEKIlq2qm6MEjhWnsmxCkKWhsJgBLc\",\n",
    "    \"AIzaSyCqNDwZ8A6r6B91TdsxUC80n4haVVWAOxY\",\n",
    "    \"AIzaSyC5W-VmRvpXCV1tbIB7JTedHf0cVmTM-FA\",\n",
    "    \"AIzaSyAzXQ4jENbZWRdjSul_xFBnLhhyW04RBxU\",\n",
    "    \"AIzaSyCCRvAA13zRUbXqH3RnY3ejZSuwfVb2g8A\",\n",
    "    \"AIzaSyD2mi7rilV-lfIMMGtsetkOeDiJivgE7Kk\",\n",
    "    \"AIzaSyBgk_IC0ZRaIqecQO2UpGlKOyl51AmMoO4\",\n",
    "    \"AIzaSyDd_61vqCZuxJOb5m8Bp9wnXSBi8lMnfLQ\",\n",
    "    \"AIzaSyBgMKdx5PDLkplaKh89VvuqsAWr29adDJE\",\n",
    "    \"AIzaSyDVoQv3uMKSJlGCFooA41vAeT6sU2hbWOM\",\n",
    "    \"AIzaSyAc_vUl_F22-VJnPisj_-aP7khzUqvDJ5o\",\n",
    "    \"AIzaSyAi5hdFBMAGEfGiYYUXWUsn5PsE0yDqzD8\",\n",
    "    \"AIzaSyAfhsvdr1em_iYqd5nkj2B_NmWvQqBXnU0\",\n",
    "    \"AIzaSyDUYAUBZggphwZTwwMbZY7ni_B810hLBMU\",\n",
    "    \"AIzaSyAMpO2h_g2pb1BzmhN7mdT_VDjtF9UI0sA\",\n",
    "    \"AIzaSyC3dRO2aRTGVIR7fERE3CUlH-j1Q5hXrac\"\n",
    "]  \n",
    "current_api_key_index = 0\n",
    "QUERY_COUNT = 0  # Track number of queries per key\n",
    "\n",
    "# YouTube API setup\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "# Paths to input/output files\n",
    "SESSION_CSV = os.path.join(LOCAL_PATH, \"control_forleo.csv\")\n",
    "OUTPUT_CSV = os.path.join(LOCAL_PATH, \"control_files.csv\")\n",
    "CACHE_FILE = os.path.join(LOCAL_PATH, \"youtube_cache.json\")\n",
    "\n",
    "# Load session data\n",
    "if not os.path.exists(SESSION_CSV):\n",
    "    raise FileNotFoundError(f\"Session CSV file not found at {SESSION_CSV}\")\n",
    "\n",
    "session_data = pd.read_csv(SESSION_CSV)\n",
    "\n",
    "# Validate required columns\n",
    "if not all(col in session_data.columns for col in ['datelocation', 'group_name']):\n",
    "    raise ValueError(\"Missing required columns in session CSV.\")\n",
    "\n",
    "session_data.dropna(subset=['datelocation', 'group_name'], inplace=True)\n",
    "\n",
    "# Generate unique search queries with variations\n",
    "ADDITIONAL_KEYWORDS = [\"full set\", \"full concert\", \"rare footage\", \"archival\", \"classic\", \"unreleased\"]\n",
    "\n",
    "SEARCH_QUERIES = [\n",
    "    f\"{row['datelocation']} {row['group_name']} live OR session OR concert OR gig OR performance OR jazz OR bebop OR set OR recording\"\n",
    "    for _, row in session_data.iterrows()\n",
    "]\n",
    "\n",
    "EXPANDED_QUERIES = []\n",
    "for query in SEARCH_QUERIES:\n",
    "    EXPANDED_QUERIES.append(query)\n",
    "    for keyword in ADDITIONAL_KEYWORDS:\n",
    "        EXPANDED_QUERIES.append(f\"{query} {keyword}\")\n",
    "\n",
    "# Remove duplicates and shuffle queries\n",
    "SEARCH_QUERIES = list(set(EXPANDED_QUERIES))\n",
    "random.shuffle(SEARCH_QUERIES)\n",
    "\n",
    "# Constants\n",
    "MAX_RESULTS = 50\n",
    "DELAY = 2  # Reduce initial delay for faster retries\n",
    "MAX_RETRIES = 3  # Limit retries to avoid long hangs\n",
    "CACHE_EXPIRY_DAYS = 7\n",
    "MAX_WORKERS = min(20, multiprocessing.cpu_count() * 2)  # Increased workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache():\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            try:\n",
    "                cache_data = json.load(f)\n",
    "                if isinstance(cache_data, dict):\n",
    "                    # Ensure each entry has a 'timestamp'\n",
    "                    return {\n",
    "                        k: v for k, v in cache_data.items()\n",
    "                        if isinstance(v, dict) and v.get(\"timestamp\", 0) > time.time() - (CACHE_EXPIRY_DAYS * 86400)\n",
    "                    }\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(\"Cache file is corrupted. Reinitializing as an empty dictionary.\")\n",
    "                return {}\n",
    "    return {}\n",
    "\n",
    "\n",
    "cache = load_cache()\n",
    "\n",
    "# Save cache to file\n",
    "def save_cache():\n",
    "    try:\n",
    "        cache_copy = cache.copy()  # Snapshot to prevent 'changed size' error\n",
    "        with open(CACHE_FILE, 'w') as f:\n",
    "            json.dump(cache_copy, f, indent=4)\n",
    "        print(\"‚úÖ Cache saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving cache: {e}\")\n",
    "        logging.error(f\"Error saving cache: {e}\")\n",
    "\n",
    "# Switch API key when quota is exceeded\n",
    "def switch_api_key():\n",
    "    global current_api_key_index\n",
    "    current_api_key_index = (current_api_key_index + 1) % len(API_KEYS)\n",
    "\n",
    "    if current_api_key_index == 0:\n",
    "        logging.error(\"‚ö†Ô∏è All API keys exhausted. Exiting script early.\")\n",
    "        print(\"‚ö†Ô∏è All API keys exhausted. Exiting script early.\")\n",
    "        save_cache()  # Ensure progress is saved before exiting\n",
    "        sys.exit(\"üö® Exiting script: All API keys exhausted.\")\n",
    "\n",
    "    print(f\"üîÑ Switching API key to: {API_KEYS[current_api_key_index]}\")\n",
    "    return API_KEYS[current_api_key_index]\n",
    "\n",
    "\n",
    "# Get YouTube API client\n",
    "def get_youtube_client():\n",
    "    global QUERY_COUNT, current_api_key_index\n",
    "\n",
    "    QUERY_COUNT += 1\n",
    "    if QUERY_COUNT >= 5:  # Rotate API key every 5 queries\n",
    "        QUERY_COUNT = 0\n",
    "        switch_api_key()\n",
    "\n",
    "    return build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEYS[current_api_key_index])\n",
    "\n",
    "# Search YouTube videos with pagination\n",
    "def search_youtube(query, max_results=MAX_RESULTS):\n",
    "    youtube = get_youtube_client()\n",
    "    video_items = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        request = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            type=\"video\",\n",
    "            maxResults=max_results,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "\n",
    "        response = request.execute()\n",
    "        video_items.extend(response.get(\"items\", []))\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "\n",
    "        if not next_page_token or len(video_items) >= 500:\n",
    "            break\n",
    "\n",
    "    print(f\"üîç Query: '{query}' ‚Üí {len(video_items)} videos found\")\n",
    "    return video_items\n",
    "\n",
    "# Retry YouTube search with exponential backoff\n",
    "def search_youtube_with_retry(query):\n",
    "    delay = DELAY\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            return search_youtube(query)\n",
    "        except HttpError as e:\n",
    "            error_message = str(e).lower()\n",
    "\n",
    "            if \"quotaexceeded\" in error_message:\n",
    "                logging.error(f\"Quota exceeded with API key {API_KEYS[current_api_key_index]}. Switching keys.\")\n",
    "                switch_api_key()\n",
    "            elif \"403\" in error_message:\n",
    "                logging.error(f\"403 Forbidden error ‚Äî possible API key ban: {e}\")\n",
    "                switch_api_key()\n",
    "            elif \"500\" in error_message or \"503\" in error_message:\n",
    "                logging.error(f\"Server error encountered, retrying: {e}\")\n",
    "                time.sleep(delay * (2 ** attempt))\n",
    "            else:\n",
    "                logging.error(f\"Unhandled API error: {e}\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            logging.error(f\"General error in API request: {e}\")\n",
    "            time.sleep(delay * (2 ** attempt))\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_data(video_items, processed_video_ids, session_terms):\n",
    "    video_data = []\n",
    "    for item in video_items:\n",
    "        video_id = item[\"id\"][\"videoId\"]\n",
    "\n",
    "        if video_id in processed_video_ids:\n",
    "            continue\n",
    "\n",
    "        title = item[\"snippet\"][\"title\"]\n",
    "        description = item[\"snippet\"][\"description\"]\n",
    "\n",
    "        if \"charlie parker\" in title.lower() or \"charlie parker\" in description.lower() \\\n",
    "                or \"bird\" in title.lower() or \"bird\" in description.lower():\n",
    "            continue\n",
    "\n",
    "        published_date = item[\"snippet\"][\"publishedAt\"]\n",
    "        video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "\n",
    "        if any(term.lower() in (title + description).lower() for term in session_terms):\n",
    "            video_data.append({\n",
    "                \"Title\": title,\n",
    "                \"Description\": description,\n",
    "                \"Published Date\": published_date,\n",
    "                \"Video URL\": video_url\n",
    "            })\n",
    "            processed_video_ids.add(video_id)\n",
    "\n",
    "    return video_data\n",
    "\n",
    "# Process a single query\n",
    "def process_query(query, processed_video_ids):\n",
    "    if query in cache:\n",
    "        video_items = cache[query][\"videos\"]\n",
    "    else:\n",
    "        video_items = search_youtube_with_retry(query)\n",
    "        cache[query] = {\"videos\": video_items, \"timestamp\": time.time()}\n",
    "\n",
    "        # Efficient cache saving ‚Äî save every 20 queries\n",
    "        if len(cache) % 20 == 0:\n",
    "            save_cache()\n",
    "\n",
    "    extracted_data = extract_video_data(\n",
    "        video_items, \n",
    "        processed_video_ids,\n",
    "        session_terms=session_data['datelocation'].tolist() + session_data['group_name'].tolist()\n",
    "    )\n",
    "\n",
    "    print(f\"üìä Extracted {len(extracted_data)} items from query: {query}\")\n",
    "    \n",
    "    # ‚úÖ Immediate CSV Save for Each Successful Query\n",
    "    if extracted_data:\n",
    "        pd.DataFrame(extracted_data).to_csv(\n",
    "            OUTPUT_CSV,\n",
    "            mode=\"a\", \n",
    "            header=not os.path.exists(OUTPUT_CSV), \n",
    "            index=False\n",
    "        )\n",
    "        print(f\"‚úÖ {len(extracted_data)} new records saved to {OUTPUT_CSV}\")\n",
    "    \n",
    "    return extracted_data if extracted_data else []  # Ensure empty results don't block data flow\n",
    "\n",
    "# Collect and save video data\n",
    "def collect_control_videos():\n",
    "    all_video_data = []\n",
    "    processed_video_ids = set()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        results = executor.map(lambda q: process_query(q, processed_video_ids), SEARCH_QUERIES)\n",
    "        for result in results:\n",
    "            all_video_data.extend(result)\n",
    "\n",
    "    print(f\"üìã Total collected data entries: {len(all_video_data)}\")\n",
    "\n",
    "    if all_video_data:  # ‚úÖ Only save if data exists\n",
    "        pd.DataFrame(all_video_data).to_csv(\n",
    "            OUTPUT_CSV, \n",
    "            mode=\"a\", \n",
    "            header=not os.path.exists(OUTPUT_CSV), \n",
    "            index=False\n",
    "        )\n",
    "        print(f\"‚úÖ Data saved to {OUTPUT_CSV} ({len(all_video_data)} new videos)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data found. CSV not created.\")\n",
    "\n",
    "# Download and convert videos to WAV\n",
    "def download_and_convert_to_wav(video_url, output_dir):\n",
    "    try:\n",
    "        # Create temp directory for downloads\n",
    "        temp_dir = os.path.join(output_dir, \"temp\")\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "        # yt-dlp options to download best audio format\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio[ext=m4a]/bestaudio/best',\n",
    "            'outtmpl': os.path.join(temp_dir, '%(title)s.%(ext)s'),\n",
    "            'quiet': True,\n",
    "            'no_warnings': True,\n",
    "            }\n",
    "\n",
    "\n",
    "        print(f\"üéß Downloading: {video_url}\")\n",
    "\n",
    "        # Download audio\n",
    "        with YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(video_url, download=True)\n",
    "            downloaded_file = ydl.prepare_filename(info)\n",
    "\n",
    "        # Ensure the downloaded file exists before attempting conversion\n",
    "        if not os.path.exists(downloaded_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file not found: {downloaded_file}\")\n",
    "\n",
    "        # Convert downloaded audio to WAV using pydub\n",
    "        print(f\"üîÑ Converting: {downloaded_file}\")\n",
    "        audio = AudioSegment.from_file(downloaded_file)\n",
    "        wav_file = os.path.join(output_dir, os.path.splitext(os.path.basename(downloaded_file))[0] + \".wav\")\n",
    "        audio.export(wav_file, format=\"wav\")\n",
    "\n",
    "        print(f\"‚úÖ Converted and saved: {wav_file}\")\n",
    "\n",
    "        # Clean up temporary files\n",
    "        os.remove(downloaded_file)\n",
    "        os.rmdir(temp_dir)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {video_url}: {e}\")\n",
    "        with open(\"error_log.txt\", \"a\") as log_file:\n",
    "            log_file.write(f\"{video_url}: {e}\\n\")\n",
    "\n",
    "# Process CSV file and download WAV files\n",
    "def download_wav_files():\n",
    "    try:\n",
    "        df = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "        if \"Video URL\" not in df.columns:\n",
    "            print(\"‚ùó Error: 'Video URL' column not found in CSV.\")\n",
    "            return\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            video_url = row.get(\"Video URL\")\n",
    "            if pd.notna(video_url) and \"youtube.com\" in video_url:\n",
    "                download_and_convert_to_wav(video_url, WAV_OUTPUT_DIR)\n",
    "\n",
    "        print(f\"‚úÖ WAV files saved to {WAV_OUTPUT_DIR}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading CSV: {e}\")\n",
    "\n",
    "# Run script\n",
    "if __name__ == \"__main__\":\n",
    "    collect_control_videos()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
